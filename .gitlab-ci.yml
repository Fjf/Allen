stages:
  - build
  - run
  - publish

check-formatting:
  stage: build
  image: debian:testing
  script:
    - apt update && apt install -y clang-format-7 python-pip git curl
    - pip install yapf==0.24.0 whichcraft
    - if [ ! -e .clang-format ] ; then
    -   curl -o .clang-format "https://gitlab.cern.ch/lhcb-parallelization/Allen/raw/master/.clang-format?inline=false"
    -   echo '.clang-format' >> .gitignore
    -   git add .gitignore
    - fi
    - curl -o lb-format "https://gitlab.cern.ch/lhcb-core/LbDevTools/raw/master/LbDevTools/SourceTools.py?inline=false"
    - python lb-format --format-patch apply-formatting.patch origin/master
  artifacts:
    paths:
      - apply-formatting.patch
    when: on_failure
    expire_in: 1 week
  allow_failure: true

.build_job: &build_job_def
  only:
    refs:
      - master
      - schedules
      - web
  stage: build
  script:
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - declare -A DEVICE_CAPABILITIES_MAP=${DEVICE_CAPABILITIES}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - SEQUENCE=${JOB_NAME_SPLIT[0]}
    - BUILD_ARCH_FLAG="-gencode arch=compute_75,code=sm_75 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_30,code=sm_30"
    - export PATH=/usr/local/cuda/bin:/cvmfs/lhcb.cern.ch/lib/contrib/CMake/3.12.1/Linux-x86_64/bin/:$PATH
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_95apython3 x86_64-centos7-gcc8-opt
    - export PATH=/cvmfs/sft.cern.ch/lcg/contrib/CMake/3.14.2/Linux-x86_64/bin:$PATH
    - mkdir build
    - cd build
    - cmake -DSEQUENCE=${SEQUENCE} -DOVERRIDE_ARCH_FLAG="${BUILD_ARCH_FLAG}" -DCPU_ARCH="ivybridge" ..
    - make -j
  artifacts:
    name: "$CI_JOB_NAME"
    expire_in: 2 hrs
    paths:
      - build*/*Allen*
      - input
  retry: 1

.run_throughput_job_no_profiling: &run_throughput_job_no_profiling_def
  only:
    refs:
      - master
      - schedules
      - web
  stage: run
  script:
    - TOPLEVEL=${PWD}
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - declare -A DEVICE_CAPABILITIES_MAP=${DEVICE_CAPABILITIES}
    - declare -A DEVICE_MEMORY_MAP=${DEVICE_MEMORY}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - DEVICE_ID=${JOB_NAME_SPLIT[0]}
    - SEQUENCE=${JOB_NAME_SPLIT[1]}
    - D_NUMBER=${DEVICE_NUMBERS_MAP[${DEVICE_ID}]}
    - D_CAPABILITY=${DEVICE_CAPABILITIES_MAP[${DEVICE_ID}]}
    - D_MEMORY=${DEVICE_MEMORY_MAP[${DEVICE_ID}]}
    - RUN_OPTIONS="-n 1000 -m 700 -r 100 -t 12 -c 0"
    - if [ "${D_MEMORY}" = "LOW" ]; then
    -   RUN_OPTIONS="-n 1000 -r 100 -t 2 -m 700 -c 0"
    - fi
    - export PATH=/usr/local/cuda/bin:/cvmfs/lhcb.cern.ch/lib/contrib/CMake/3.12.1/Linux-x86_64/bin/:$PATH
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_95apython3 x86_64-centos7-gcc8-opt
    - export PATH=/cvmfs/sft.cern.ch/lcg/contrib/CMake/3.14.2/Linux-x86_64/bin:$PATH
    - mkdir output_${DEVICE_ID}
    - cd build
    - ls
    - export LD_LIBRARY_PATH=${PWD}:$LD_LIBRARY_PATH
    - CUDA_VISIBLE_DEVICES=${D_NUMBER} ./Allen -f /localprojects/shared/201907/minbias_mag_down ${RUN_OPTIONS} 2>&1 | tee ../output_${DEVICE_ID}/output.txt
  artifacts:
    name: "$CI_JOB_NAME"
    expire_in: 2 hrs
    paths:
      - output_*
  allow_failure: true
  retry: 1

.run_throughput_job: &run_throughput_job_def
  only:
    refs:
      - master
      - schedules
      - web
  stage: run
  script:
    - TOPLEVEL=${PWD}
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - declare -A DEVICE_CAPABILITIES_MAP=${DEVICE_CAPABILITIES}
    - declare -A DEVICE_MEMORY_MAP=${DEVICE_MEMORY}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - DEVICE_ID=${JOB_NAME_SPLIT[0]}
    - SEQUENCE=${JOB_NAME_SPLIT[1]}
    - D_NUMBER=${DEVICE_NUMBERS_MAP[${DEVICE_ID}]}
    - D_CAPABILITY=${DEVICE_CAPABILITIES_MAP[${DEVICE_ID}]}
    - D_MEMORY=${DEVICE_MEMORY_MAP[${DEVICE_ID}]}
    - RUN_OPTIONS="-n 1000 -m 700 -r 100 -t 12 -c 0"
    - if [ "${D_MEMORY}" = "LOW" ]; then
    -   RUN_OPTIONS="-n 1000 -r 100 -t 2 -m 700 -c 0"
    - fi
    - export PATH=/usr/local/cuda/bin:/cvmfs/lhcb.cern.ch/lib/contrib/CMake/3.12.1/Linux-x86_64/bin/:$PATH
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_95apython3 x86_64-centos7-gcc8-opt
    - export PATH=/cvmfs/sft.cern.ch/lcg/contrib/CMake/3.14.2/Linux-x86_64/bin:$PATH
    - mkdir output_${DEVICE_ID}
    - cd build
    - ls
    - export LD_LIBRARY_PATH=${PWD}:$LD_LIBRARY_PATH
    - CUDA_VISIBLE_DEVICES=${D_NUMBER} ./Allen -f /localprojects/shared/201907/minbias_mag_down ${RUN_OPTIONS} 2>&1 | tee ../output_${DEVICE_ID}/output.txt
    - CUDA_VISIBLE_DEVICES=${D_NUMBER} /usr/local/cuda-10.0/bin/nvprof ./Allen -f /localprojects/shared/201907/minbias_mag_down ${RUN_OPTIONS} 2>&1 | tee ../output_${DEVICE_ID}/profiler_output.txt
    - python3 ${TOPLEVEL}/checker/plotting/extract_algo_breakdown.py -d ${TOPLEVEL}
  artifacts:
    name: "$CI_JOB_NAME"
    expire_in: 2 hrs
    paths:
      - output_*
  allow_failure: true
  retry: 1

.throughput_cli_plot_job: &publish_algo_breakdown_plot_def
  only:
    refs:
      - master
      - schedules
      - web
  stage: publish
  script:
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - declare -A DEVICE_CAPABILITIES_MAP=${DEVICE_CAPABILITIES}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - DEVICE_ID=${JOB_NAME_SPLIT[0]}
    - SEQUENCE=${JOB_NAME_SPLIT[1]}
    - D_NUMBER=${DEVICE_NUMBERS_MAP[${DEVICE_ID}]}
    - D_CAPABILITY=${DEVICE_CAPABILITIES_MAP[${DEVICE_ID}]}
    - export PATH=/usr/local/cuda/bin:/cvmfs/lhcb.cern.ch/lib/contrib/CMake/3.12.1/Linux-x86_64/bin/:$PATH
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_95apython3 x86_64-centos7-gcc8-opt
    - export PATH=/cvmfs/sft.cern.ch/lcg/contrib/CMake/3.14.2/Linux-x86_64/bin:$PATH
    - python3 checker/plotting/csv_plotter.py -t "Algorithm Breakdown for ${SEQUENCE}" -u "%" -x 40 -m ${MATTERMOST_KEY} output_${DEVICE_ID}/algo_breakdown.csv
    - python3 checker/plotting/csv_plotter.py -t "Algorithm Groups for ${SEQUENCE}" -u "%" -m ${MATTERMOST_KEY} output_${DEVICE_ID}/algo_summary.csv

.throughput_throughput_job: &publish_throughput_job_def
  only:
    refs:
      - master
      - schedules
      - web
  stage: publish
  script:
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - SEQUENCE=${JOB_NAME_SPLIT[1]}
    - cat output_*/output.txt | grep --color=none "device" | sed 's/.*:\ [0-9]*\,\ //' > devices.txt
    - cat output_*/output.txt | grep --color=none "events/s" | awk '{ print $1; }' > throughputs.txt
    - cat devices.txt
    - cat throughputs.txt
    - paste -d, devices.txt throughputs.txt > devices_throughputs.csv
    - cat devices_throughputs.csv
    - python3 checker/plotting/csv_plotter.py -t "Throughputs for ${SEQUENCE}" -u "kHz" -x 70 -s 1e-3 -m ${MATTERMOST_KEY} devices_throughputs.csv
    - python3 checker/plotting/post_telegraf.py -d . -s ${SEQUENCE} -b ${CI_COMMIT_REF_NAME} 

.throughput_speedup_job: &publish_speedup_job_def
  only:
    refs:
      - master
      - schedules
      - web
  stage: publish
  script:
    - cat output_*/output.txt | grep --color=none "device" | sed 's/.*:\ [0-9]*\,\ //' > devices.txt
    - cat output_*/output.txt | grep --color=none "events/s" | awk '{ print $1; }' > throughputs.txt
    - cat devices.txt 
    - cat throughputs.txt
    - paste -d, devices.txt throughputs.txt > devices_throughputs.csv
    - cat devices_throughputs.csv
    - python3 checker/plotting/csv_plotter.py -n -t "Speedup across GPUs" -u "x" -x 30 -m ${MATTERMOST_KEY} devices_throughputs.csv

# Build for all platforms
DefaultSequence:build:
  <<: *build_job_def
  tags:
    - t4

# DataPreparation:build:
#   <<: *build_job_def
#   tags:
#     - rtx2080ti

# Run on all platforms
# DefaultSequence
rtx2080ti:DefaultSequence:run_throughput:
  <<: *run_throughput_job_def
  tags:
    - rtx2080ti
  dependencies:
    - DefaultSequence:build

v100:DefaultSequence:run_throughput:
  <<: *run_throughput_job_def
  tags:
    - v100
  dependencies:
    - DefaultSequence:build

t4:DefaultSequence:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - t4
  dependencies:
    - DefaultSequence:build

gtx780ti:DefaultSequence:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - gtx780ti
  dependencies:
    - DefaultSequence:build

gtx10606g:DefaultSequence:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - gtx10606g
  dependencies:
    - DefaultSequence:build

gtx980:DefaultSequence:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - gtx980
  dependencies:
    - DefaultSequence:build

rtx6000:DefaultSequence:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - rtx6000
  dependencies:
    - DefaultSequence:build

gtx680:DefaultSequence:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - gtx680
  dependencies:
    - DefaultSequence:build

gtxtitanx:DefaultSequence:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - gtxtitanx
  dependencies:
    - DefaultSequence:build

gtx670:DefaultSequence:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - gtx670
  dependencies:
    - DefaultSequence:build

gtx1080ti:DefaultSequence:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - gtx1080ti
  dependencies:
    - DefaultSequence:build

# # DataPreparation
# # Run on all platforms
# rtx2080ti:DataPreparation:run_throughput:
#   <<: *run_throughput_job_def
#   tags:
#     - rtx2080ti
#   dependencies:
#     - DataPreparation:build

# v100:DataPreparation:run_throughput:
#   <<: *run_throughput_job_def
#   tags:
#     - v100
#   dependencies:
#     - DataPreparation:build

# t4:DataPreparation:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - t4
#   dependencies:
#     - DataPreparation:build

# gtx780ti:DataPreparation:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - gtx780ti
#   dependencies:
#     - DataPreparation:build

# gtx10606g:DataPreparation:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - gtx10606g
#   dependencies:
#     - DataPreparation:build

# gtx980:DataPreparation:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - gtx980
#   dependencies:
#     - DataPreparation:build

# gtx680:DataPreparation:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - gtx680
#   dependencies:
#     - DataPreparation:build

# gtxtitanx:DataPreparation:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - gtxtitanx
#   dependencies:
#     - DataPreparation:build

# gtx670:DataPreparation:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - gtx670
#   dependencies:
#     - DataPreparation:build

# gtx1080ti:DataPreparation:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - gtx1080ti
#   dependencies:
#     - DataPreparation:build

# Publish

# DefaultSequence
# Plain results to mattermost
rtx2080ti:DefaultSequence:publish_algo_breakdown_plot:
  <<: *publish_algo_breakdown_plot_def
  tags:
    - gpu
  dependencies:
    - rtx2080ti:DefaultSequence:run_throughput

throughput:DefaultSequence:publish_throughput:
  <<: *publish_throughput_job_def
  tags:
    - gpu
  dependencies:
    - rtx2080ti:DefaultSequence:run_throughput
    - rtx6000:DefaultSequence:run_throughput
    - v100:DefaultSequence:run_throughput
    - t4:DefaultSequence:run_throughput
    - gtx10606g:DefaultSequence:run_throughput
    - gtx680:DefaultSequence:run_throughput
    - gtxtitanx:DefaultSequence:run_throughput
    - gtx670:DefaultSequence:run_throughput
    - gtx780ti:DefaultSequence:run_throughput
    - gtx1080ti:DefaultSequence:run_throughput
    - gtx980:DefaultSequence:run_throughput

# The "gpu" tag is to require python3 essentially
speedup:DefaultSequence:publish_speedup:
  <<: *publish_speedup_job_def
  tags:
    - gpu
  dependencies:
    - rtx2080ti:DefaultSequence:run_throughput
    - rtx6000:DefaultSequence:run_throughput
    - v100:DefaultSequence:run_throughput
    - t4:DefaultSequence:run_throughput
    - gtx10606g:DefaultSequence:run_throughput
    - gtx680:DefaultSequence:run_throughput
    - gtx980:DefaultSequence:run_throughput
    - gtxtitanx:DefaultSequence:run_throughput
    - gtx670:DefaultSequence:run_throughput
    - gtx780ti:DefaultSequence:run_throughput
    - gtx1080ti:DefaultSequence:run_throughput
    - gtx980:DefaultSequence:run_throughput

# # Data Preparation
# rtx2080ti:DataPreparation:publish_algo_breakdown_plot:
#   <<: *publish_algo_breakdown_plot_def
#   tags:
#     - gpu
#   dependencies:
#     - rtx2080ti:DataPreparation:run_throughput

# throughput:DataPreparation:publish_throughput:
#   <<: *publish_throughput_job_def
#   tags:
#     - gpu
#   dependencies:
#     - rtx2080ti:DataPreparation:run_throughput
#     - v100:DataPreparation:run_throughput
#     - t4:DataPreparation:run_throughput
#     - gtx10606g:DataPreparation:run_throughput
#     - gtx680:DataPreparation:run_throughput
#     - gtxtitanx:DataPreparation:run_throughput
#     - gtx670:DataPreparation:run_throughput
#     - gtx780ti:DataPreparation:run_throughput
#     - gtx1080ti:DataPreparation:run_throughput
#     - gtx980:DataPreparation:run_throughput

# speedup:DataPreparation:publish_speedup:
#   <<: *publish_speedup_job_def
#   tags:
#     - gpu
#   dependencies:
#     - rtx2080ti:DataPreparation:run_throughput
#     - v100:DataPreparation:run_throughput
#     - t4:DataPreparation:run_throughput
#     - gtx10606g:DataPreparation:run_throughput
#     - gtx680:DataPreparation:run_throughput
#     - gtx980:DataPreparation:run_throughput
#     - gtxtitanx:DataPreparation:run_throughput
#     - gtx670:DataPreparation:run_throughput
#     - gtx780ti:DataPreparation:run_throughput
#     - gtx1080ti:DataPreparation:run_throughput
#     - gtx980:DataPreparation:run_throughput
