stages:
  - check-env # Ensures the CI environment is valid
  - build     # Builds all projects
  - run       # Runs the tests across architectures
  - test      # Runs various tests of the software
  - publish   # Publishes the results of the tests and runs in channels and grafana

check-env:
  stage: check-env
  except:
    - /.*/@lhcb/Allen
  script:
    - |
      echo "The Allen CI depends on custom GitLab runners and therefore tests"
      echo "running on forks will fail. Please create a branch in the main"
      echo "repository at https://gitlab.cern.ch/lhcb/Allen/"
    - exit 1

check-formatting:
  stage: build
  image: gitlab-registry.cern.ch/lhcb-docker/style-checker
  script:
    - if [ ! -e .clang-format ] ; then
    -   curl -o .clang-format "https://gitlab.cern.ch/lhcb-parallelization/Allen/raw/master/.clang-format?inline=false"
    -   echo '.clang-format' >> .gitignore
    -   git add .gitignore
    - fi
    - curl -o lb-format "https://gitlab.cern.ch/lhcb-core/LbDevTools/raw/master/LbDevTools/SourceTools.py?inline=false"
    - python lb-format --format-patch apply-formatting.patch origin/master
  artifacts:
    paths:
      - apply-formatting.patch
    when: on_failure
    expire_in: 1 week
  allow_failure: true

.build_job: &build_job_def
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: build
  script:
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - TARGET=${JOB_NAME_SPLIT[0]}
    - SEQUENCE=${JOB_NAME_SPLIT[1]}
    - BUILD_TYPE=${JOB_NAME_SPLIT[2]}
    - ADDITIONAL_OPTIONS=${JOB_NAME_SPLIT[3]}
    - OVERRIDE_CUDA_ARCH_FLAG="-gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75"
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-gcc8-opt
    - source /cvmfs/sft.cern.ch/lcg/contrib/cuda/10.2/x86_64-centos7/setup.sh
    - mkdir build_${TARGET}
    - cd build_${TARGET}
    - cmake -DSTANDALONE=ON -G Ninja -DTARGET_DEVICE=${TARGET} -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DSEQUENCE=${SEQUENCE} -DCPU_ARCH=ivybridge -DOVERRIDE_CUDA_ARCH_FLAG="${OVERRIDE_CUDA_ARCH_FLAG}" ${ADDITIONAL_OPTIONS} ..
    - ninja -j 8
  artifacts:
    expire_in: 2 hrs
    paths:
      - build*/*Allen*
      - build*/Sequence.json
      - input
  retry: 1

.build_job: &build_test_job_def
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: build
  script:
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - TARGET=${JOB_NAME_SPLIT[0]}
    - SEQUENCE=${JOB_NAME_SPLIT[1]}
    - BUILD_TYPE=${JOB_NAME_SPLIT[2]}
    - ADDITIONAL_OPTIONS=${JOB_NAME_SPLIT[3]}
    - OVERRIDE_CUDA_ARCH_FLAG="-gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75"
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-gcc8-opt
    - source /cvmfs/sft.cern.ch/lcg/contrib/cuda/10.2/x86_64-centos7/setup.sh
    - mkdir build_${TARGET}
    - cd build_${TARGET}
    - cmake -DSTANDALONE=ON -G Ninja -DBUILD_TESTS=ON -DTARGET_DEVICE=${TARGET} -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DSEQUENCE=${SEQUENCE} -DCPU_ARCH=ivybridge -DOVERRIDE_CUDA_ARCH_FLAG="${OVERRIDE_CUDA_ARCH_FLAG}" ${ADDITIONAL_OPTIONS} ..
    - ninja -j 8
  artifacts:
    expire_in: 2 hrs
    paths:
      - build*/*
      - input
  retry: 1

.build_clang_job: &build_clang_job_def
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: build
  script:
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - TARGET=${JOB_NAME_SPLIT[0]}
    - SEQUENCE=${JOB_NAME_SPLIT[1]}
    - BUILD_TYPE=${JOB_NAME_SPLIT[2]}
    - ADDITIONAL_OPTIONS=${JOB_NAME_SPLIT[3]}
    - OVERRIDE_CUDA_ARCH_FLAG="-gencode=arch=compute_70,code=sm_70 -gencode=arch=compute_75,code=sm_75"
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-clang8-opt
    - source /cvmfs/sft.cern.ch/lcg/contrib/cuda/10.2/x86_64-centos7/setup.sh
    - mkdir build_${TARGET}
    - cd build_${TARGET}
    - cmake -DSTANDALONE=ON -G Ninja -DTARGET_DEVICE=${TARGET} -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DSEQUENCE=${SEQUENCE} -DCPU_ARCH=ivybridge -DOVERRIDE_CUDA_ARCH_FLAG="${OVERRIDE_CUDA_ARCH_FLAG}" ${ADDITIONAL_OPTIONS} ..
    - ninja -j 8
  artifacts:
    expire_in: 2 hrs
    paths:
      - build*/*Allen*
      - build*/Sequence.json
      - input
  retry: 1

docker_image:build:
  stage: build
  only:
    refs:
      - master
  tags:
    - docker-image-build
  script: "echo 'Building Allen dev docker image"
  variables:
    TO: $CI_REGISTRY_IMAGE:latest

.run_physics_efficiency_job: &run_physics_efficiency_job_def
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: run
  script:
    - TOPLEVEL=${PWD}
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - declare -A DEVICE_MEMORY_MAP=${DEVICE_MEMORY}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=':' read -ra CI_RUNNER_DESCRIPTION_SPLIT <<< "${CI_RUNNER_DESCRIPTION}"
    - IFS=${PREVIOUS_IFS}
    - DEVICE_ID=${JOB_NAME_SPLIT[0]}
    - TARGET=${JOB_NAME_SPLIT[1]}
    - SEQUENCE=${JOB_NAME_SPLIT[2]}
    - D_NUMBER=${CI_RUNNER_DESCRIPTION_SPLIT[1]}
    - RUN_OPTIONS="-n 1000 -m 700"
    - export PATH=$PATH:/usr/local/cuda/bin
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-gcc8-opt
    - mkdir validation_output
    - ln -s validation_output output # Needed by Root build
    - cd build_${TARGET}
    - ls
    - export LD_LIBRARY_PATH=${PWD}:$LD_LIBRARY_PATH
    - CUDA_VISIBLE_DEVICES=${D_NUMBER} ./Allen -f /scratch/dcampora/allen_data/201907/bsphiphi_mag_down ${RUN_OPTIONS} 2>&1 | tee ../validation_output/bsphiphi_${DEVICE_ID}.txt
  artifacts:
    expire_in: 2 hrs
    paths:
      - validation_output/*
  allow_failure: true
  retry: 1

.run_throughput_job_no_profiling: &run_throughput_job_no_profiling_def
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: run
  script:
    - TOPLEVEL=${PWD}
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - declare -A DEVICE_MEMORY_MAP=${DEVICE_MEMORY}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=':' read -ra CI_RUNNER_DESCRIPTION_SPLIT <<< "${CI_RUNNER_DESCRIPTION}"
    - IFS=${PREVIOUS_IFS}
    - DEVICE_ID=${JOB_NAME_SPLIT[0]}
    - TARGET=${JOB_NAME_SPLIT[1]}
    - SEQUENCE=${JOB_NAME_SPLIT[2]}
    - D_NUMBER=${CI_RUNNER_DESCRIPTION_SPLIT[1]}
    - D_MEMORY=${DEVICE_MEMORY_MAP[${DEVICE_ID}]}
    - RUN_OPTIONS="-n 1000 -m 700 -r 100 -t 12 -c 0"
    - if [ "${D_MEMORY}" = "LOW" ]; then
    -   RUN_OPTIONS="-n 1000 -r 100 -t 2 -m 700 -c 0"
    - fi
    - export PATH=$PATH:/usr/local/cuda/bin
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-gcc8-opt
    - mkdir output_${DEVICE_ID}
    - cd build_${TARGET}
    - ls
    - export LD_LIBRARY_PATH=${PWD}:$LD_LIBRARY_PATH
    - CUDA_VISIBLE_DEVICES=${D_NUMBER} ./Allen -f /scratch/dcampora/allen_data/201907/minbias_mag_down ${RUN_OPTIONS} 2>&1 | tee ../output_${DEVICE_ID}/output.txt
  artifacts:
    expire_in: 2 hrs
    paths:
      - output_*
  allow_failure: true
  retry: 1

.run_throughput_job_no_profiling_cpu: &run_throughput_job_no_profiling_def_cpu
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: run
  script:
    - TOPLEVEL=${PWD}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - DEVICE_ID=${JOB_NAME_SPLIT[0]}
    - TARGET=${JOB_NAME_SPLIT[1]}
    - SEQUENCE=${JOB_NAME_SPLIT[2]}
    - THREADS=$(lscpu | egrep "^CPU\(s\):.*[0-9]+$" --color=none | awk '{ print $2; }')
    - RUN_OPTIONS="-n 1000 -m 700 -r 10 -t ${THREADS} -c 0" # If we ever execute on other CPUs
    - export PATH=$PATH:/usr/local/cuda/bin
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-gcc8-opt
    - mkdir output_${DEVICE_ID}
    - cd build_${TARGET}
    - ls
    - export LD_LIBRARY_PATH=${PWD}:$LD_LIBRARY_PATH
    - ./Allen -f /scratch/dcampora/allen_data/201907/minbias_mag_down ${RUN_OPTIONS} 2>&1 | tee ../output_${DEVICE_ID}/output.txt
  artifacts:
    expire_in: 2 hrs
    paths:
      - output_*
  allow_failure: true
  retry: 1

.run_throughput_job: &run_throughput_job_def
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: run
  script:
    - TOPLEVEL=${PWD}
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - declare -A DEVICE_MEMORY_MAP=${DEVICE_MEMORY}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=':' read -ra CI_RUNNER_DESCRIPTION_SPLIT <<< "${CI_RUNNER_DESCRIPTION}"
    - IFS=${PREVIOUS_IFS}
    - DEVICE_ID=${JOB_NAME_SPLIT[0]}
    - TARGET=${JOB_NAME_SPLIT[1]}
    - SEQUENCE=${JOB_NAME_SPLIT[2]}
    - D_NUMBER=${CI_RUNNER_DESCRIPTION_SPLIT[1]}
    - D_MEMORY=${DEVICE_MEMORY_MAP[${DEVICE_ID}]}
    - RUN_OPTIONS="-n 1000 -m 700 -r 100 -t 12 -c 0"
    - if [ "${D_MEMORY}" = "LOW" ]; then
    -   RUN_OPTIONS="-n 1000 -r 100 -t 2 -m 700 -c 0"
    - fi
    - export PATH=$PATH:/usr/local/cuda/bin
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-gcc8-opt
    - mkdir output_${DEVICE_ID}
    - cd build_${TARGET}
    - ls
    - export LD_LIBRARY_PATH=${PWD}:$LD_LIBRARY_PATH
    - CUDA_VISIBLE_DEVICES=${D_NUMBER} ./Allen -f /scratch/dcampora/allen_data/201907/minbias_mag_down ${RUN_OPTIONS} 2>&1 | tee ../output_${DEVICE_ID}/output.txt
    - NVPROF_TMPDIR=`mktemp -d`
    - CUDA_VISIBLE_DEVICES=${D_NUMBER} TMPDIR=${NVPROF_TMPDIR} nvprof ./Allen -f /scratch/dcampora/allen_data/201907/minbias_mag_down ${RUN_OPTIONS} 2>&1 | tee ../output_${DEVICE_ID}/profiler_output.txt
    - rm -rf ${NVPROF_TMPDIR}
    - python3 ${TOPLEVEL}/checker/plotting/extract_algo_breakdown.py -d ${TOPLEVEL}
  artifacts:
    expire_in: 2 hrs
    paths:
      - output_*
  allow_failure: true
  retry: 1

.throughput_cli_plot_job: &publish_algo_breakdown_plot_def
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: publish
  script:
    - declare -A DEVICE_NUMBERS_MAP=${DEVICE_NUMBERS}
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - DEVICE_ID=${JOB_NAME_SPLIT[0]}
    - SEQUENCE=${JOB_NAME_SPLIT[1]}
    - export PATH=$PATH:/usr/local/cuda/bin
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-gcc8-opt
    - python3 checker/plotting/csv_plotter.py -t "Algorithm Breakdown of sequence __${SEQUENCE}__, branch _${CI_COMMIT_REF_NAME}_" -u "%" -x 30 -m ${MATTERMOST_KEY} output_${DEVICE_ID}/algo_breakdown.csv

test_physics_efficiency:
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: test
  script:
    - TOPLEVEL=${PWD}
    - ls validation_output
    - ls ${TOPLEVEL}/test/reference
    - cd validation_output
    - for i in $( ls ); do echo "Checking ${i}"; tail -n97 ${i} | head -n94 > efficiency_${i}; diff -u ${TOPLEVEL}/test/reference/${i} efficiency_${i} | tee ${i}_diff || true; done
    - cat *_diff > alldiffs
    - if [ -s alldiffs ]; then echo "Differences were found against reference files."; exit 1; else echo "No differences found against reference files."; exit 0; fi
  dependencies:
    - geforcertx2080ti:CUDA:hlt1_pp_default:run_physics_efficiency
    - x862630v4:CPU:hlt1_pp_default:run_physics_efficiency
  tags:
    - cvmfs
  allow_failure: true

# run_built_tests:
#   only:
#     refs:
#       - master
#       - schedules
#       - web
#       - merge_requests
#   stage: test
#   script:
#     - cd build_CUDA
#     - ctest -V
#   dependencies:
#     - CUDA:hlt1_pp_default:Debug::build
#   allow_failure: true

.publish_throughput_job: &publish_throughput_job_def
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: publish
  script:
    - PREVIOUS_IFS=${IFS}
    - IFS=':' read -ra JOB_NAME_SPLIT <<< "${CI_JOB_NAME}"
    - IFS=${PREVIOUS_IFS}
    - SEQUENCE=${JOB_NAME_SPLIT[1]}
    - BREAKDOWN_DEVICE_ID=${JOB_NAME_SPLIT[2]}
    - cat output_*/output.txt | grep --color=none "device" | sed 's/.*:\ [0-9]*\,\ //' > devices.txt
    - cat output_*/output.txt | grep --color=none "events/s" | awk '{ print $1; }' > throughputs.txt
    - cat devices.txt
    - cat throughputs.txt
    - paste -d, devices.txt throughputs.txt > devices_throughputs.csv
    - cat devices_throughputs.csv
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-gcc8-opt
    - python3 checker/plotting/post_combined_message.py -l "Throughput of [sequence __${SEQUENCE}__, branch _${CI_COMMIT_REF_NAME}_](https://gitlab.cern.ch/lhcb/Allen/pipelines/${CI_PIPELINE_ID})" -m ${MATTERMOST_KEY} -t devices_throughputs.csv -b output_${BREAKDOWN_DEVICE_ID}/algo_breakdown.csv
    - python3 checker/plotting/post_telegraf.py -d . -s ${SEQUENCE} -b ${CI_COMMIT_REF_NAME}

.throughput_speedup_job: &publish_speedup_job_def
  only:
    refs:
      - master
      - schedules
      - web
      - merge_requests
  stage: publish
  script:
    - cat output_*/output.txt | grep --color=none "device" | sed 's/.*:\ [0-9]*\,\ //' > devices.txt
    - cat output_*/output.txt | grep --color=none "events/s" | awk '{ print $1; }' > throughputs.txt
    - cat devices.txt
    - cat throughputs.txt
    - paste -d, devices.txt throughputs.txt > devices_throughputs.csv
    - cat devices_throughputs.csv
    - source /cvmfs/sft.cern.ch/lcg/views/setupViews.sh LCG_97python3 x86_64-centos7-gcc8-opt
    - python3 checker/plotting/csv_plotter.py -n -t "Speedup across GPUs, branch _${CI_COMMIT_REF_NAME}_" -u "x" -x 30 -m ${MATTERMOST_KEY} devices_throughputs.csv

# =====
# Build
# =====

CUDA:hlt1_pp_default:RelWithDebInfo::build:
  <<: *build_job_def
  tags:
    - cvmfs

CUDA:hlt1_pp_default:Debug:-DUSE_ROOT=ON:build:
  <<: *build_job_def
  tags:
    - cvmfs

CPU:hlt1_pp_default:Debug::build:
  <<: *build_test_job_def
  tags:
    - cvmfs

CPU:hlt1_pp_default:Debug:-DUSE_ROOT=ON:build:
  <<: *build_job_def
  tags:
    - cvmfs

CPU:hlt1_pp_default:RelWithDebInfo::build:
  <<: *build_job_def
  tags:
    - cvmfs

CPU:hlt1_pp_default:RelWithDebInfo::build_clang:
  <<: *build_clang_job_def
  tags:
    - cvmfs

# ===
# Run
# ===

# Throughput runs

geforcertx2080ti:CUDA:hlt1_pp_default:run_throughput:
  <<: *run_throughput_job_def
  tags:
    - geforcertx2080ti
  dependencies:
    - CUDA:hlt1_pp_default:RelWithDebInfo::build

quadrortx6000:CUDA:hlt1_pp_default:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - quadrortx6000
  dependencies:
    - CUDA:hlt1_pp_default:RelWithDebInfo::build

teslav100:CUDA:hlt1_pp_default:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - teslav100
  dependencies:
    - CUDA:hlt1_pp_default:RelWithDebInfo::build

teslat4:CUDA:hlt1_pp_default:run_throughput:
  <<: *run_throughput_job_no_profiling_def
  tags:
    - teslat4
  dependencies:
    - CUDA:hlt1_pp_default:RelWithDebInfo::build

# Note: Add -gencode=arch=compute_61,code=sm_61
#       to the configuration if these cards are enabled.
 
# geforcegtx10606g:CUDA:hlt1_pp_default:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - geforcegtx10606g
#   dependencies:
#     - CUDA:hlt1_pp_default:RelWithDebInfo::build

# geforcegtx1080ti:CUDA:hlt1_pp_default:run_throughput:
#   <<: *run_throughput_job_no_profiling_def
#   tags:
#     - geforcegtx1080ti
#   dependencies:
#     - CUDA:hlt1_pp_default:RelWithDebInfo::build

x862630v4:CPU:hlt1_pp_default:run_throughput_cpu:
  <<: *run_throughput_job_no_profiling_def_cpu
  tags:
    - x862630v4
  dependencies:
    - CPU:hlt1_pp_default:RelWithDebInfo::build

# Physics efficiency runs

geforcertx2080ti:CUDA:hlt1_pp_default:run_physics_efficiency:
  <<: *run_physics_efficiency_job_def
  tags:
    - geforcertx2080ti
  dependencies:
    - CUDA:hlt1_pp_default:RelWithDebInfo::build

x862630v4:CPU:hlt1_pp_default:run_physics_efficiency:
  <<: *run_physics_efficiency_job_def
  tags:
    - x862630v4
  dependencies:
    - CPU:hlt1_pp_default:RelWithDebInfo::build

# Test runs

geforcertx2080ti:CUDA:hlt1_pp_default:run_physics_efficiency_debug:
  <<: *run_physics_efficiency_job_def
  tags:
    - geforcertx2080ti
  dependencies:
    - CUDA:hlt1_pp_default:Debug:-DUSE_ROOT=ON:build

x86:CPU:hlt1_pp_default:run_physics_efficiency_debug:
  <<: *run_physics_efficiency_job_def
  tags:
    - x86
  dependencies:
    - CPU:hlt1_pp_default:Debug::build

x86:CPU:hlt1_pp_default:run_physics_efficiency_debug_root:
  <<: *run_physics_efficiency_job_def
  tags:
    - x86
  dependencies:
    - CPU:hlt1_pp_default:Debug:-DUSE_ROOT=ON:build

# =======
# Publish
# =======

throughput:hlt1_pp_default:geforcertx2080ti:publish_throughput:
  <<: *publish_throughput_job_def
  tags:
    - x86
  dependencies:
    - geforcertx2080ti:CUDA:hlt1_pp_default:run_throughput
    - quadrortx6000:CUDA:hlt1_pp_default:run_throughput
    - teslat4:CUDA:hlt1_pp_default:run_throughput
    - x862630v4:CPU:hlt1_pp_default:run_throughput_cpu
    - teslav100:CUDA:hlt1_pp_default:run_throughput
    # - geforcegtx1080ti:CUDA:hlt1_pp_default:run_throughput
    # - geforcegtx10606g:CUDA:hlt1_pp_default:run_throughput
